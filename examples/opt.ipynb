{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a8fc6d",
   "metadata": {},
   "source": [
    "# Levenberg-Marquardt Optimization with AADC Derivatives\n",
    "This notebook demonstrates using Levenberg-Marquardt optimization with both SciPy and the\n",
    "AADC implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deac2f64",
   "metadata": {},
   "source": [
    "## 1. Standard Implementation with SciPy\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788bd1d6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "import aadc\n",
    "\n",
    "%matplotlib inline\n",
    "plt.ioff()  # Turn off interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8416515",
   "metadata": {},
   "source": [
    "Now we'll define our residual function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a099d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the function to be minimized (residuals)\n",
    "def residual_func(params, x_data, y_data):\n",
    "    # Example: fitting y = a * exp(-b * x) + c\n",
    "    a, b, c = params\n",
    "    y_model = a * np.exp(-b * x_data) + c\n",
    "    return y_model - y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b04457c",
   "metadata": {},
   "source": [
    "Let's generate some sample data for our optimization problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aee7e8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Generate some sample data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "x_data = np.linspace(0, 10, 10)\n",
    "true_params = [5.0, 0.5, 1.0]\n",
    "y_true = true_params[0] * np.exp(-true_params[1] * x_data) + true_params[2]\n",
    "noise = np.random.normal(0, 0.2, x_data.shape)\n",
    "y_data = y_true + noise\n",
    "print(\"x_data:\", x_data)\n",
    "print(\"y_data:\", y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5418c8f",
   "metadata": {},
   "source": [
    "Now we'll create an optimization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization(x_data, y_data):\n",
    "    # Initial guess for parameters\n",
    "    initial_params = [1.0, 1.0, 0.0]\n",
    "\n",
    "    # Perform Levenberg-Marquardt optimization\n",
    "    result = least_squares(\n",
    "        residual_func,\n",
    "        initial_params,\n",
    "        method='lm',\n",
    "        args=(x_data, y_data),\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    return result, initial_params\n",
    "\n",
    "# Run the optimization\n",
    "result, initial_params = run_optimization(x_data, y_data)\n",
    "\n",
    "# Print results\n",
    "print(\"True parameters:\", true_params)\n",
    "print(\"Optimal parameters:\", result.x)\n",
    "print(\"Cost:\", result.cost)\n",
    "print(\"Success:\", result.success)\n",
    "\n",
    "# Plot the optimization results for SciPy implementation\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the noisy data points\n",
    "plt.scatter(x_data, y_data, color='blue', alpha=0.6, label='Noisy data')\n",
    "\n",
    "# Plot the true function\n",
    "plt.plot(x_data, y_true, 'g-', linewidth=2, label='True function')\n",
    "\n",
    "# Plot the initial guess\n",
    "initial_guess = initial_params[0] * np.exp(-initial_params[1] * x_data) + initial_params[2]\n",
    "plt.plot(x_data, initial_guess, 'r--', linewidth=2, label='Initial guess')\n",
    "\n",
    "# Plot the optimized function\n",
    "a_opt, b_opt, c_opt = result.x\n",
    "y_opt = a_opt * np.exp(-b_opt * x_data) + c_opt\n",
    "plt.plot(x_data, y_opt, 'k-', linewidth=2, label='Optimized fit')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('SciPy Levenberg-Marquardt Optimization Results')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add text box with optimized parameters\n",
    "param_text = f'Optimized parameters:\\na = {a_opt:.4f}\\nb = {b_opt:.4f}\\nc = {c_opt:.4f}'\n",
    "plt.text(6, 4, param_text, bbox=dict(facecolor='white', alpha=0.8))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6bc556",
   "metadata": {},
   "source": [
    "## 2. Implementation with AADC\n",
    "Note that objective function is using y_data internally and derivatives must be calculated with\n",
    "respect to y_data. AADC.least_squares() function doesn't require Jacobian w.r.t. y_data, it will\n",
    "be calculated automatically. See Automatic IFT publication for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf585db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AADC version\n",
    "\n",
    "func = aadc.Kernel()\n",
    "func.start_recording()\n",
    "\n",
    "# Define the residual function with AADC inputs\n",
    "iy_data = aadc.ndarray.AADCArray([aadc.idouble(y) for y in y_data])\n",
    "iy_data_arg = [y.mark_as_input() for y in iy_data]\n",
    "\n",
    "def objective(params):\n",
    "    return residual_func(params, x_data, iy_data)\n",
    "\n",
    "aadc_result = aadc.least_squares(objective, initial_params)\n",
    "print(\"AADC Optimal parameters:\", aadc_result.x)\n",
    "\n",
    "x_res = [a.mark_as_output() for a in aadc_result.x]\n",
    "func.stop_recording()\n",
    "func.print_passive_extract_locations()\n",
    "\n",
    "inputs = {y_arg: y for y_arg, y in zip(iy_data_arg, y_data)}\n",
    "request = {x_res[i]: iy_data_arg for i in range(len(x_res))}\n",
    "res = aadc.evaluate(func, request, inputs)\n",
    "\n",
    "print(\"True parameters:\", true_params)\n",
    "print(\"Scipy Optimal parameters:\", result.x)\n",
    "print(\"AADC Kernel Optimal parameters:\", [res[0][x_res[i]] for i in range(len(x_res))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea2589",
   "metadata": {},
   "source": [
    "## 3. Comparing Derivatives\n",
    "Let's compare the derivatives computed by both methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b5555",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"AADC Kernel Derivatives param 0:\", res[1][x_res[0]])\n",
    "print(\"AADC Kernel Derivatives param 1:\", res[1][x_res[1]])\n",
    "print(\"AADC Kernel Derivatives param 2:\", res[1][x_res[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce345f",
   "metadata": {},
   "source": [
    "Now let's verify the derivatives using the finite difference method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cccf0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to calculate derivatives using finite differences\n",
    "def calculate_finite_diff_derivatives(params, x_data, y_data, epsilon=1e-6):\n",
    "    derivatives = []\n",
    "\n",
    "    for i in range(len(params)):\n",
    "        param_derivatives = []\n",
    "\n",
    "        for j in range(len(y_data)):\n",
    "            # Create modified y_data with a bump in one element\n",
    "            y_bumped = y_data.copy()\n",
    "            y_bumped[j] += epsilon\n",
    "\n",
    "            # Run optimization with bumped data\n",
    "            bumped_result, _ = run_optimization(x_data, y_bumped)\n",
    "\n",
    "            # Calculate derivative (change in parameter / change in y)\n",
    "            derivative = (bumped_result.x[i] - params[i]) / epsilon\n",
    "            param_derivatives.append(derivative)\n",
    "\n",
    "        derivatives.append(param_derivatives)\n",
    "\n",
    "    return derivatives\n",
    "\n",
    "# Calculate finite difference derivatives for all parameters\n",
    "# Use %%capture to suppress the output\n",
    "print(\"Calculating finite difference derivatives (this may take some time)...\")\n",
    "fd_derivatives = calculate_finite_diff_derivatives(result.x, x_data, y_data)\n",
    "\n",
    "# Extract AADC derivatives in a format suitable for comparison\n",
    "aadc_derivatives = []\n",
    "for i in range(len(result.x)):\n",
    "    # Extract derivatives from the AADC result\n",
    "    aadc_deriv_i = np.array([res[1][x_res[i]][y_arg][0] for y_arg in iy_data_arg])\n",
    "    aadc_derivatives.append(aadc_deriv_i)\n",
    "\n",
    "    print(f\"Parameter {i} derivatives:\")\n",
    "    print(f\" AADC: {aadc_deriv_i}\")\n",
    "    print(f\" Finite diff: {fd_derivatives[i]}\")\n",
    "    print(f\" Mean difference: {np.mean(np.abs(aadc_deriv_i - np.array(fd_derivatives[i])))}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12a2eb8",
   "metadata": {},
   "source": [
    "## 4. Combined Visualization\n",
    "Let's visualize both the optimization results and parameter sensitivities in a single figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined figure with optimization results and parameter sensitivities\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Optimization Results (Top)\n",
    "ax1 = plt.subplot2grid((4, 1), (0, 0), rowspan=1)\n",
    "\n",
    "# Plot the noisy data points\n",
    "ax1.scatter(x_data, y_data, color='blue', alpha=0.6, label='Noisy data')\n",
    "\n",
    "# Plot the true function\n",
    "ax1.plot(x_data, y_true, 'g-', linewidth=2, label='True function')\n",
    "\n",
    "# Plot the initial guess\n",
    "initial_guess = initial_params[0] * np.exp(-initial_params[1] * x_data) + initial_params[2]\n",
    "ax1.plot(x_data, initial_guess, 'r--', linewidth=2, label='Initial guess')\n",
    "\n",
    "# Plot the optimized function\n",
    "a_opt, b_opt, c_opt = result.x\n",
    "y_opt = a_opt * np.exp(-b_opt * x_data) + c_opt\n",
    "ax1.plot(x_data, y_opt, 'k-', linewidth=2, label='Optimized fit')\n",
    "\n",
    "# Add labels and legend\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Levenberg-Marquardt Optimization Results')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add text box with optimized parameters\n",
    "param_text = f'Optimized parameters:\\na = {a_opt:.4f}\\nb = {b_opt:.4f}\\nc = {c_opt:.4f}'\n",
    "ax1.text(0.05, 0.5, param_text, transform=ax1.transAxes,\n",
    "         bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "# Plots 2-4: Parameter Sensitivities (Bottom three rows)\n",
    "param_names = ['a', 'b', 'c']\n",
    "for i in range(len(result.x)):\n",
    "    ax = plt.subplot2grid((4, 1), (i+1, 0), rowspan=1)\n",
    "\n",
    "    ax.plot(x_data, aadc_derivatives[i], 'bo-', label='AADC derivative')\n",
    "    ax.plot(x_data, fd_derivatives[i], 'ro--', label='Finite diff derivative')\n",
    "\n",
    "    ax.set_title(f'Parameter {param_names[i]} sensitivity to data points')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel(f'd{param_names[i]}/dy')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94f2b9",
   "metadata": {},
   "source": [
    "# Automatic Implicit Function Theorem - Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a2719f",
   "metadata": {},
   "source": [
    "## Paper Details\n",
    "\n",
    "* Title: Automatic Implicit Function Theorem\n",
    "* Authors: Dmitri Goloubentsev, Evgeny Lakshtanov, Vladimir Piterbarg\n",
    "* Affiliation: Matlogica, Universidade de Aveiro, NatWest Markets, Imperial College London\n",
    "* Published: December 14, 2021 (Revised: May 31, 2022)\n",
    "* SSRN ID: 3984964\n",
    "* URL: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3984964"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
